---
layout: post
title: "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"
date: 2025-05-09
categories: [cv-nlp-pe] # 또는 [research, nlp] 또는 [research, computer-vision]
use_math: true
---

[논문 링크](https://arxiv.org/pdf/2401.14196)

> **오픈소스 모델과 폐쇄형(Closed-source) 모델 간의 성능 차이**가 여전히 중요한 과제로 남아 있다(Li et al., 2023; Nijkamp et al., 2022; Roziere et al., 2023; Wang et al., 2021). 특히, **폐쇄형 모델(OpenAI GPT-4, Gemini 등)은 강력하지만, 연구자 및 개발자들에게 접근이 제한적**이라는 점이 문제로 지적된다. 이에 대한 해결책으로 **DeepSeek-Coder 시리즈**를 소개한다. 이 모델 시리즈는 **1.3B(13억)에서 33B(330억)까지 다양한 크기의 오픈소스 코드 모델**을 포함하며, 각 크기별로 기본(Base) 모델과 지시(Instruct) 모델이 존재한다.

### 1. 서론

요즘 코드 생성 인공지능이 엄청 발전하고 있다. 하지만 **오픈소스 코드 LLM(대형 언어 모델)**과 폐쇄형(상용) 모델 사이에는 여전히 큰 성능 차이가 존재한다.
GPT-4, Gemini 같은 상용 모델은 강력하지만, 일반 개발자나 연구자들은 쉽게 쓸 수 없다.

"DeepSeek-Coder는 이 문제를 해결하기 위해 등장한 오픈소스 코드 특화 LLM 시리즈다."

### 2.DeepSeek-Coder 모델 구조(Architecture)

<strong>Transformer 기반 구조 </strong>

- 1.3B, 6.7B, 33B 등 여러 크기로 제공
- 각 모델은 Base(기본), Instruct(지시형) 버전이 있다.
- 16000 토큰까지 긴 입력을 처리할 수 있음
  모델 크기: 1.3B / 6.7B / 33B

Transformer 기반 디코더 전용 구조
RoPE 위치 임베딩 사용
FlashAttention v2 및 Grouped-Query Attention(GQA) (33B 모델에 적용)

#### 최적화 및 학습 정책

Optimizer: AdamW (β₁=0.9, β₂=0.95)
Warm-up: 2000 스텝
3단계 러닝레이트 스케줄링: 각 단계마다 √(1/10)씩 감소
32K Vocabulary BPE Tokenizer (HuggingFace 기반)

### 데이터셋(Data Collection & Processing)

데이터 전처리는 다음의 5단계로 구성:

1. **크롤링**: 2023년 2월 이전 GitHub 공개 저장소 대상
2. **규칙 기반 필터링**: StarCoder 규칙 기반으로 낮은 품질 제거
3. **의존성 파싱**: `import`, `include` 등 정규식을 통해 파일 간 의존성 추출
4. **Repo 단위 중복 제거**: 저장소 전체를 하나의 샘플로 보고 중복 제거
5. **품질 스크리닝 + 테스트 셋 디컨태미네이션**: 컴파일, 정적분석, n-gram 기반 HumanEval 등 벤치마크 제거

> 중복 제거는 **파일 단위가 아닌 저장소 단위**로 진행해 구조를 보존했고, HumanEval/MBPP 등 벤치마크 누출을 방지하는 **n-gram 수준 필터링**을 적용했다.

![dataset creation](images\bighack\d1.png)

### 의존성 기반 정렬 학습

단순한 파일 레벨 학습에서 벗어나, **파일 간 의존성을 고려한 순서 정렬(topological sort)**을 수행하여 실제 소프트웨어 구조에 더 가까운 데이터로 학습한다.

- 정규식을 사용해 Python(`import`), C#(`using`), C(`include`) 등 의존성 파악
- 연결된 파일들 간 정렬을 통해 **컨텍스트를 보존**
- 각 파일 시작에 **파일 경로를 주석으로 삽입**하여 문맥 반영

### Pretraining

코드뿐만 아니라 코드 관련 문서, 영어/중국어 자연어도 일부 포함해서 학습했다.

1. **Next Token Prediction**: 일반적인 다음 토큰 예측

   > 2. FIM(Fill-In-Middle) 기법을 적용했다. → 코드의 중간을 비워두고, 그 부분을 채우는 방식으로 학습시켜 코드 자동 완성 능력이 강화됐다. 레포지토리 단위로 학습하여, 여러 파일에 걸친 코드 생성도 잘 할 수 있게 됐다.

```txt
<｜fim_start｜>Prefix<｜fim_hole｜>Suffix<｜fim_end｜>Middle<|eos_token|>
```

### Instruction Tuning

- Base 모델에 추가로 지시형 튜닝을 적용해서 Instruct 모델을 만들었다.
  => **자연어 명령을 주면 그에 맞는 코드를 생성하는 능력이 크게 향상**
- ## 지시형 튜닝이란?
  - 사용자의 명령이나 지시에 따라 제대로 반응하도록 만드는 파인튜닝 방법
  - "이렇게 해줘", "이 코드 설명해줘", "파이썬으로 정렬 함수 짜줘" 같은 구체적인 명령을 주면, 모델이 그에 맞는 답을 잘 내놓도록 추가로 학습시키는 과정이다.
  - **"지시(Instruction)"와 "입력(Input, 있을 경우)", 그리고 "기대하는 출력(Output)"이 짝지어진 데이터셋을 사용해서 모델을 다시 학습** -> 다양한 실전 명령에 훨씬 더 잘 반응하게 된다

### Benchmark Evaluation(실험)

HumanEval, MBPP, DS-1000 등 다양한 코드 생성 벤치마크에서 실험했다.
DeepSeek-Coder-Base 33B: 모든 오픈소스 코드 LLM 중 최고 성능을 기록했다.
DeepSeek-Coder-Instruct 33B: GPT-3.5 Turbo보다 더 좋은 성능을 보였다.
6.7B 모델: CodeLlama-34B보다 더 좋은 결과를 냈다.

FIM 성능, 파일 간 코드 완성, 수학 추론 등 다양한 테스트에서 강력함을 입증

### Analysis & Ablation Study

**FIM 기법이 실제로 코드 자동 완성 성능을 얼마나 올려주는지 분석**
**Repository 단위 학습이 파일 간 코드 생성에 미치는 영향도 분석**

### Conclusion

DeepSeek-Coder는 오픈소스 코드 LLM의 새로운 기준을 제시했다.
다양한 크기와 강력한 성능, 그리고 repo 단위 학습, FIM 기법 등 혁신적인 접근이 특징이다. 앞으로 더 큰 일반 LLM을 기반으로 한 코드 특화 모델이 나올 가능성도 보여줬다.개발자와 연구자 모두에게 강력한 오픈소스 코드 생성 도구를 제공한다는 점에서 의미가 크다.

> DeepSeek-Coder는 오픈소스 코드 생성 LLM 중 최강 성능을 자랑하며, 폐쇄형 모델과의 격차를 빠르게 좁히고 있는 혁신적인 모델이다.
