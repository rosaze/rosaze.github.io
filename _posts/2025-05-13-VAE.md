---
layout: post
title: "Auto-Encoding Variational Bayes"
date: 2025-05-13
categories: [cv-nlp-pe] # 또는 [research, nlp] 또는 [research, computer-vision]
use_math: true
---

### [논문 링크](https://arxiv.org/abs/1312.6114)

# VAE (Variational Auto Encoder)

### generation with Neural Networks

inputs: Pixel level. -> 원하는 인풋 픽셀이 특정 이미지를 만들 수 있도록 Generative Neural Networks 설계. -/

**Generative Neural Networks**

- Inputs이 예시로 pixel level일때 고해상도 -> 학습이 어렵다 (예측하기 어렵다)
<p style="text-align: center;">따라서</p>
- inputs: 데이터 안에 숨어있는 변수들 . latent variable(high level description)
  **latent variable 의 pros**
- only train low dimension information -> easy to estimate
- can control the generative process
  즉 Generative Neural networks 의 목표는 데이터셋에 있을법한 잠재 변수들을 하나하나 매핑하는 것. GMM 에서는 아예 정의를 하고 시작했지만 ,뉴럴 네트워크는 빅데이터에 있는잠재 변수들을 학습을 통해 찾아내는 것.머신러닝 기반의 생성모델보다 훨씬 더 데이터를 잘 찾아냄. "찾고 -> 생성한다" 매핑은 잘 찾아진 데이터를 각각의 타겟 도메인에 맞게 생성을 한다. 네트워크를 학습을 한다.

_ input data 를 잘 나타낼 수 있는 latent variable를 어떻게 잘 찾을건가?_

gan 이이랑 vae 차이. diffusion dms vae 랑 거의 똑같ㅇㅁ .vae는 한 스텝만에 찾는데 디퓨전은 이걸 1000번을 함 -> 우리가 알고있는 gaussian 으로 함. 각 프로세슷를 이해할떄 latent var을 어떻게 찾는지.

in terms of Encoder
**AutoEncoder**: explore the latent variable as deterministic (일대일로 매핑) code .
**variational autoencoder**: explore the latent variable as stochastic (확률적으로 매핑) code.확률저그로 샘플링 된 형태로 만들어진다. 왜 autoencoder가 생성형이 아니지 ? . 차이점은decoder 가 아니라 encoder에 있음 .

in terms of decoder

- mapping function from latent variable to output data -> Autoencoder= variaitional autoencoder

### AutoEncoder

autoencoder= encoder + decoder . neural networks for compressing and reconstructing data.
데이터를 z 로 압축하고 암축된 데이터로부터 원래 데이터를 복원한다.
encoder: 잠재변수 z로 압축.

- 예시: input data 의 차원: 256*256*3 -> 1\*2 차원으로 압축, 이 압축하는 네트워크가 인코드. 이미지 데이터면 인코더를 cnn 으로 쌓고 오디오면 rnn 등, transformer 도 됨.블락이 뭐든지 간에에 인코더는 압축.
- 예시: 이미지를 encoder 에 넣는 순간 z 를 갖고, 각각은 deterministic.
  중요: **실제로는 latent variable 의 값의 의미를 알 수 는 없다.**
