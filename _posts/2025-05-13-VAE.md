---
layout: post
title: "Auto-Encoding Variational Bayes"
date: 2025-05-13
categories: [cv-nlp-pe] # ë˜ëŠ” [research, nlp] ë˜ëŠ” [research, computer-vision]
use_math: true
---

### [ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/1312.6114)

# VAE (Variational Auto Encoder)

### generation with Neural Networks

inputs: Pixel level. -> ì›í•˜ëŠ” ì¸í’‹ í”½ì…€ì´ íŠ¹ì • ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ Generative Neural Networks ì„¤ê³„. -/

**Generative Neural Networks**

- Inputsì´ ì˜ˆì‹œë¡œ pixel levelì¼ë•Œ ê³ í•´ìƒë„ -> í•™ìŠµì´ ì–´ë µë‹¤ (ì˜ˆì¸¡í•˜ê¸° ì–´ë µë‹¤)
<p style="text-align: center;">ë”°ë¼ì„œ</p>
- inputs: ë°ì´í„° ì•ˆì— ìˆ¨ì–´ìˆëŠ” ë³€ìˆ˜ë“¤ . latent variable(high level description)
  **latent variable ì˜ pros**
- only train low dimension information -> easy to estimate
- can control the generative process
  ì¦‰ Generative Neural networks ì˜ ëª©í‘œëŠ” ë°ì´í„°ì…‹ì— ìˆì„ë²•í•œ ì ì¬ ë³€ìˆ˜ë“¤ì„ í•˜ë‚˜í•˜ë‚˜ ë§¤í•‘í•˜ëŠ” ê²ƒ. GMM ì—ì„œëŠ” ì•„ì˜ˆ ì •ì˜ë¥¼ í•˜ê³  ì‹œì‘í–ˆì§€ë§Œ ,ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ëŠ” ë¹…ë°ì´í„°ì— ìˆëŠ”ì ì¬ ë³€ìˆ˜ë“¤ì„ í•™ìŠµì„ í†µí•´ ì°¾ì•„ë‚´ëŠ” ê²ƒ.ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ì˜ ìƒì„±ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ë” ë°ì´í„°ë¥¼ ì˜ ì°¾ì•„ëƒ„. "ì°¾ê³  -> ìƒì„±í•œë‹¤" ë§¤í•‘ì€ ì˜ ì°¾ì•„ì§„ ë°ì´í„°ë¥¼ ê°ê°ì˜ íƒ€ê²Ÿ ë„ë©”ì¸ì— ë§ê²Œ ìƒì„±ì„ í•œë‹¤. ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµì„ í•œë‹¤.

_ input data ë¥¼ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” latent variableë¥¼ ì–´ë–»ê²Œ ì˜ ì°¾ì„ê±´ê°€?_

gan ì´ì´ë‘ vae ì°¨ì´. diffusion dms vae ë‘ ê±°ì˜ ë˜‘ê°™ã…‡ã… .vaeëŠ” í•œ ìŠ¤í…ë§Œì— ì°¾ëŠ”ë° ë””í“¨ì „ì€ ì´ê±¸ 1000ë²ˆì„ í•¨ -> ìš°ë¦¬ê°€ ì•Œê³ ìˆëŠ” gaussian ìœ¼ë¡œ í•¨. ê° í”„ë¡œì„¸ìŠ·ë¥¼ ì´í•´í• ë–„ latent varì„ ì–´ë–»ê²Œ ì°¾ëŠ”ì§€.

in terms of Encoder
**AutoEncoder**: explore the latent variable as deterministic (ì¼ëŒ€ì¼ë¡œ ë§¤í•‘) code .
**variational autoencoder**: explore the latent variable as stochastic (í™•ë¥ ì ìœ¼ë¡œ ë§¤í•‘) code.í™•ë¥ ì €ê·¸ë¡œ ìƒ˜í”Œë§ ëœ í˜•íƒœë¡œ ë§Œë“¤ì–´ì§„ë‹¤. ì™œ autoencoderê°€ ìƒì„±í˜•ì´ ì•„ë‹ˆì§€ ? . ì°¨ì´ì ì€decoder ê°€ ì•„ë‹ˆë¼ encoderì— ìˆìŒ .
for building the generative neral networks based on autoencoder

1. we define the latent var

- ì¸ì½”ë”ë¡œë¶€í„° í•™ìŠµì„ ì‹œí‚¤ëŠ” ê²ƒ.

2. we embed the input data into the latent space
3. we generate output data from explored latent space

in terms of decoder

- mapping function from latent variable to output data -> Autoencoder= variaitional autoencoder

## AutoEncoder

Autoencoder = Encoder + Decoder

- AutoencoderëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì ì¬ ê³µê°„(latent space)ìœ¼ë¡œ ì••ì¶•í–ˆë‹¤ê°€ ë‹¤ì‹œ ë³µì›í•˜ëŠ” ì‹ ê²½ë§ êµ¬ì¡°ì´ë‹¤. ë¹„ì§€ë„ í•™ìŠµì— ì‚¬ìš©ë˜ë©°, ì…ë ¥ê³¼ ì¶œë ¥ì´ ë™ì¼í•œ í˜•íƒœë¥¼ ê°€ì§„ë‹¤.
- neural networks for compressing and reconstructing data.
  train encoder and decoder with reconstruction loss(MSE loss)
  ë°ì´í„°ë¥¼ z ë¡œ ì••ì¶•í•˜ê³  ì•”ì¶•ëœ ë°ì´í„°ë¡œë¶€í„° ì›ë˜ ë°ì´í„°ë¥¼ ë³µì›í•œë‹¤.
  encoder: ì˜ ëŒ€í‘œí• ìˆ˜ ìˆëŠ” íŠ¹ì§•ë“¤ë¡œ ì ì¬ë³€ìˆ˜ zë¡œ ì••ì¶•.

<div style="background-color:rgba(100, 92, 247, 0.23); padding: 20px; border-radius: 10px;">
  <h3 style="margin-top: 0;">êµ¬ì„±</h3>
  <p>AutoencoderëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ëœë‹¤:<br>

- $Encoder Q(x)$ : ì…ë ¥ xë¥¼ ì ì¬ ë³€ìˆ˜ zë¡œ ì¸ì½”ë”© <br>
- $Decoder P(z)$: ì ì¬ ë³€ìˆ˜ zë¥¼ ì›ë˜ ì…ë ¥ê³¼ ë¹„ìŠ·í•œ yë¡œ ë””ì½”ë”©<br>
- $Loss$: ì…ì¶œë ¥ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì¬êµ¬ì„± ì†ì‹¤ ì‚¬ìš©</p>
<p>

$x \in \mathbb{R}^n \quad \text{(ì…ë ¥ ë°ì´í„°)}$
$z = Q(x) \in \mathbb{R}^d \quad \text{(ì ì¬ í‘œí˜„, \( d < n \))}$
$y = P(z) = P(Q(x)) \quad \text{(ë³µì›ëœ ì¶œë ¥)}$

<hr>
ì†ì‹¤í•¨ìˆ˜: $\mathcal{L}_{AE} = \sum_{x \in D} \mathcal{L}(x, y) = \sum_{x \in D} \mathcal{L}(x, P(Q(x)))$
<br>ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ëŠ” -> MSEì´ë‹¤.
$\mathcal{L}_{\text{reconstruction}}(x, y) = \|x - y\|^2$

</p>
</div>

**encoder**

- ì˜ˆì‹œ:
  - ì…ë ¥ ì´ë¯¸ì§€ê°€ ì˜ˆë¥¼ ë“¤ì–´ 256x256x3(ê°€ë¡œ 256, ì„¸ë¡œ 256, RGB 3ì±„ë„) ì§œë¦¬
  - ì´ê±¸ **ì ì¬ ê³µê°„(latent space)**ì˜ ì•„ì£¼ ì‘ì€ ë²¡í„°, ì˜ˆë¥¼ ë“¤ì–´ 1x2 í¬ê¸°ë¡œ ì••ì¶•. ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ë²¡í„°ë¡œ "ì••ì¶•"í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ë°”ë¡œ **ì¸ì½”ë”(encoder)**
  - ì´ë¯¸ì§€ë¼ë©´ **CNN (Convolutional Neural Network)**ì„ ì£¼ë¡œ ì¨ì„œ ì¸ì½”ë”ë¥¼ êµ¬ì„±í•˜ê³ , ì˜¤ë””ì˜¤ë‚˜ ì‹œí€€ìŠ¤ ë°ì´í„°ë©´ **RNN (Recurrent Neural Network)**ì„ ì“°ê¸°
- ì˜ˆì‹œ:
  - ì´ë¯¸ì§€ë¥¼ ì¸ì½”ë”ì— ë„£ìœ¼ë©´ ë°”ë¡œ ì ì¬ ë²¡í„° zê°€ ë‚˜ì˜´.
  - ê°ê°ì€ deterministic. : xë¥¼ ë„£ìœ¼ë©´ í•­ìƒ ê°™ì€ zê°€ ë‚˜ì˜¨ë‹¤ëŠ” ëœ».(í™•ë¥ ì ì´ ì•„ë‹ˆë¼ í™•ì •ì )
  - ì¤‘ìš”: **ì‹¤ì œë¡œëŠ” latent variable ì˜ ê°’ì˜ ì˜ë¯¸ë¥¼ ì•Œ ìˆ˜ ëŠ” ì—†ë‹¤.**

**decoder:**
(zë¥¼ ë°›ì•„ì„œ ì›ë˜ì˜ xì²˜ëŸ¼ ë³´ì´ê²Œ) ì›ë˜ ì´ë¯¸ì§€ë¡œ ë³µì›í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ . í•™ìŠµì´ ì˜ ë˜ë©´, P(z)ê°€ xë‘ ê±°ì˜ ë˜‘ê°™ì•„ì§€ë„ë¡ RECONSTRUCT ê°€ëŠ¥

```text
x (256x256x3) â”€â–¶ Encoder â”€â–¶ z (1x2) â”€â–¶ Decoder â”€â–¶ y (ë³µì›ëœ ì´ë¯¸ì§€)
```

### Autoencoder is not a generative model. WHY?

í”½ì…€ ëŒ€ì‘ì´ 1:1 ë ˆë²¨ì¼ë•Œ ë‹¨ì 
ê³ í•´ìƒë„ì¼ìˆ˜ë¡ í•™ìŠµë„ ì–´ë µê³ , ìƒì„±ë„ ì–´ë µë‹¤
ì»¨íŠ¸ë¡¤ì´ ì–´ë µë‹¤. ë§¤í•‘ì´ ì–´ë µë‹¤ .
ìœ„ ë‘ ë‹¨ì ì„ latent var ì„ ã…‡ã…‡ í•¨ìœ¼ë¡œì¨ í•´ê²°.
**goal of generative neural networks**

1. latent variable ì´ input data ë¥¼ ì˜ ë‚˜íƒ€ë‚´ë„ë¡ íƒìƒ‰í•´ì•¼ í•œë‹¤.
2. train the generative neural networks to synthesize the output data. ì›ë˜ ë°ì´í„° ì˜ ì°¾ê¸°.

â†’ **ì…ë ¥ ë°ì´í„° ì—†ì´ latent variable zë¥¼ ì •ì˜í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸.**
â†’ **ì¦‰, zëŠ” ë°˜ë“œì‹œ ì…ë ¥ ë°ì´í„° xë¥¼ í†µí•´ì„œë§Œ ì–»ì„ ìˆ˜ ìˆìŒ.**

zëŠ” ì…ë ¥ xì—ì„œ í•­ìƒ ê°™ì€ ê°’ìœ¼ë¡œ ì••ì¶•ëœë‹¤.
â†’ ì¦‰, zëŠ” ê³ ì •ëœ ê°’ (deterministic)
â†’ ê·¸ë˜ì„œ ì¶œë ¥ yëŠ” í•­ìƒ ì…ë ¥ xì™€ ê°™ìŒ (ê·¸ì € ë³µì›í•˜ëŠ” ê²ƒë¿)

AutoencoderëŠ” ì£¼ë¡œ ì°¨ì› ì¶•ì†Œ(compression) ìš©ë„
â†’ ì˜ˆ: PCAì™€ ìœ ì‚¬í•œ ì—­í• 

\*\*But VAE (Variational Autoencoder)ëŠ” ë‹¤ë¥´ë‹¤\*\*

- VAEëŠ” input data ì—†ì–´ë„ latent variable ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤ë©´, ìƒì„±ëª¨ë¸ì„ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ë¹Œë”©í•  ìˆ˜ ìˆìŒ.ì¦‰, zê°€ ì–´ë–¤ ë¶„í¬(ì˜ˆ: ì •ê·œë¶„í¬)ë¥¼ ë”°ë¥¸ë‹¤ê³  ì •ì˜ë§Œ í•˜ë©´, ìƒ˜í”Œë§ì„ í†µí•´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ.

ìˆ˜í•™ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³´ì§€ ì•Šê³ ëŠ” ì˜ë¯¸ ìˆëŠ” zë¥¼ ì •ì˜í•  ìˆ˜ ì—†ìŒ.
â†’ ê·¸ë˜ì„œ ì¸ì½”ë”ê°€ í•„ìš”í•¨: ì…ë ¥ ë°ì´í„° ë¶„í¬ë¥¼ ë³´ê³  zê°€ ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¼ì•¼ í• ì§€ í•™ìŠµí•¨. ì¦‰, ë°ì´í„°ì™€ latent variable ê°„ì˜ ì—°ê´€ì„±ì„ ë§Œë“¤ì–´ ì£¼ëŠ” ê²Œ ì¸ì½”ë”ì˜ ì—­í• 

- ë””ì½”ë”ëŠ” latent variable zë¥¼ ë°›ì•„ì„œ ì›ë˜ ì…ë ¥ ë°ì´í„° xë¥¼ ë³µì›í•˜ë ¤ëŠ” ì—­í• ì„ í•œë‹¤.Autoencoderì™€ VAE ëª¨ë‘ ë””ì½”ë”ëŠ” ì…ë ¥ê³¼ ìœ ì‚¬í•œ ì¶œë ¥ì„ ë§Œë“¤ì–´ë‚´ì§€ë§Œ, zì˜ ì„±ê²©ì´ ì™„ì „íˆ ë‹¤ë¥´ë‹¤.

## Variational Autoencoder

latent variable as stochastic code

autoencoderëŠ” ê·¸ëƒ¥ ì••ì¶•ì‹œí‚¨ ê²ƒìœ¼ë¡œ, ì°¨ì›ì„ ì •ì˜í•  ìˆ˜ ì—†ë‹¤. ë°˜ë©´ VAEì˜ latent spaceì—ì„œëŠ” ê° í¬ì¸íŠ¸ê°€ í•˜ë‚˜ì˜ Gaussian ë¶„í¬ë¡œ í‘œí˜„ëœë‹¤.
ì´ëŠ” ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œ, ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê¸°ë°˜ì´ ëœë‹¤.ì¦‰, VAEëŠ” ì°¨ì› ì¶•ì†Œë¥¼ í•˜ë©´ì„œë„ í™•ë¥ ì  ë¶„í¬ë¥¼ í•™ìŠµí•˜ì—¬ ìƒì„± ê°€ëŠ¥ì„±ì„ í™•ë³´í•œë‹¤.

- variational: ë³€ë¶„. ì–´ë–¤ í•¨ìˆ˜ì˜ í˜•íƒœë¥¼ ì¡°ê¸ˆì”© ë³€í™”ì‹œí‚¤ë©´ì„œ , ê·¸ ê²°ê³¼ê°€ ìµœì ì´ ë˜ëŠ” ì¡°ê±´ì„ ì°¾ëŠ” ë°©ë²•. **vaeëŠ” ë°ì´í„°ì˜ ë³µì¡í•œ ë¶„í¬ë¥¼ ë‹¨ìˆœí•œ ë¶„í¬ë¡œ ê·¼ì‚¬í•˜ëŠ” ëª¨ë¸**
- í•œ ìŠ¤í… ë§Œì— ê·¼ì‚¬
- ë³µì¡í•œ ë¶„í¬ -> ì¸ì½”ë” -> ì •ì˜í• ìˆ˜ìˆëŠ” ë¶„í¬(ê·¼ì‚¬ )

---

**For Building the generative neural networks based on Autoencoder**

### 1. (stochastic) latent variable ì •ì˜

ìš°ë¦¬ëŠ” latent variable `z`ê°€ **ì •ê·œë¶„í¬(Gaussian distribution)** ì—ì„œ ìƒ˜í”Œë§ëœë‹¤ê³  ê°€ì •í•  ìˆ˜ ìˆë‹¤.

- ì¦‰, ìˆ˜í•™ì ìœ¼ë¡œ `z ~ N(0, I)` ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì •ì˜ëœë‹¤.

### 2. embed the input data into the latent space

ë¬´ì‘ìœ„ Gaussian ë¶„í¬ë§Œìœ¼ë¡œëŠ” ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì˜ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤.

- ë”°ë¼ì„œ, **Encoder ë„¤íŠ¸ì›Œí¬**ë¥¼ í•™ìŠµí•˜ì—¬ ì…ë ¥ ë°ì´í„°ë¥¼ latent spaceì— ì˜ ë§ëŠ” Gaussian ë¶„í¬ë¡œ **ê·¼ì‚¬**í•˜ê²Œ í•œë‹¤.
- EncoderëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ í†µí•´ Gaussian ë¶„í¬ì˜ **íŒŒë¼ë¯¸í„° `Î¼`(mean), `Ïƒ`(standard deviation)** ë¥¼ ì¶”ì •í•œë‹¤. ì•Œì•„ì„œ loss functionì„ ë§Œë“¤ì–´ì£¼ë©´ neural network ê°€ ì•Œì•„ì„œ ì°¾ì•„ì¤Œ. weight ë“¤ì´ ì•Œì•„ì„œ í•™ìŠµí•´ì¤Œ.
- ê²°ê³¼ì ìœ¼ë¡œ, **ê³ ì°¨ì› ì…ë ¥ ë°ì´í„°ë¥¼ ì‘ì€ latent ë³€ìˆ˜(ìˆ¨ê²¨ì§„ ë³€ìˆ˜)ì˜ ì •ê·œë¶„í¬**ë¡œ ë§¤í•‘í•˜ëŠ” ê³¼ì •ì´ë‹¤.

### 3. generate output data from explored latent space

í•™ìŠµëœ latent spaceì—ì„œ `z ~ N(Î¼, Ïƒ)`ë¥¼ ìƒ˜í”Œë§í•˜ê³ , ì´ë¥¼ **Decoder**ì— ì…ë ¥í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤. **ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±**í•˜ê±°ë‚˜, ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³µì›í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.

ëª©í‘œ: log P<sub>Î¸</sub>(x)ì˜ ìµœëŒ€í™”

- ìš°ë¦¬ê°€ **ì‹¤ì œë¡œ ì°¾ê³  ì‹¶ì€ ê²ƒ**ì€ ë°”ë¡œ `log PÎ¸(x)`
- VAEì˜ í•™ìŠµ ëª©ì ì€ ì£¼ì–´ì§„ ê´€ì¸¡ ë°ì´í„° `x`ì— ëŒ€í•œ í™•ë¥  `PÎ¸(x)`ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ í™•ë¥ ì€ ì ì¬ ë³€ìˆ˜ `z`ì— ëŒ€í•´ ì ë¶„ëœ í˜•íƒœë¡œ ì£¼ì–´ì§€ë¯€ë¡œ ì§ì ‘ ê³„ì‚°ì´ ì–´ë µë‹¤.

$
\[
P_\theta(x) = \int p_\theta(x \mid z) p(z) \, dz
\]
$

> ë”°ë¼ì„œ, ì´ë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ Variational Inference ê¸°ë²•ì„ ì ìš©í•˜ì—¬ **Evidence Lower Bound (ELBO)** ë¥¼ ë„ì…í•œë‹¤.

#### ELBO ìœ ë„ ê³¼ì •

ì„ì˜ ë¶„í¬ $\( q\_\phi(z \mid x) \)$ë¥¼ ë„ì…í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì „ê°œí•œë‹¤:
<img src="images/lecture/a3.png" alt="ê³ ì–‘ì´" width="700" height="300">

### Variational Autoencoder with code

Model(Encoder,Decoder)

1. ì…ë ¥ â†’ ì ì¬ ê³µê°„ ì¸ì½”ë”© $Encoder \( Q(x) \)$

ì…ë ¥ ë²¡í„° x ëŠ” Fully Connected Layer (MLP)ì™€ ReLUë¥¼ ê±°ì³ ì ì¬ ê³µê°„(latent space)ìœ¼ë¡œ ì¸ì½”ë”©ëœë‹¤.

ì ì¬ ê³µê°„ì€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ë©°, EncoderëŠ” í‰ê·  ë²¡í„° $\( \mu \)$ì™€ ë¡œê·¸ ë¶„ì‚° ë²¡í„° $\( \log(\sigma^2) \)$ë¥¼ ì¶œë ¥í•œë‹¤.

ìƒ˜í”Œë§ëœ zëŠ” reparameterization trickì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤:
$
\[
z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)
\]
$
ìƒ˜í”Œë§ëœ zëŠ” ë‹¤ì‹œ MLPì™€ ReLUë¥¼ ê±°ì³ ì›ë˜ ì…ë ¥ê³¼ ìœ ì‚¬í•œ yë¥¼ ë³µì›í•œë‹¤.

DecoderëŠ” $\( P(z) \)$ë¥¼ í†µí•´ $\( x \)$ì˜ ë¶„í¬ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¬êµ¬ì„±í•œë‹¤:

$
\[
y \approx \hat{x} = P(z)
\]
$

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super(VAE, self).__init__()

        # ğŸ”· Encoder
        self.fc1 = nn.Linear(input_dim, hidden_dim)             # ì…ë ¥ x â†’ ì€ë‹‰ ë²¡í„° h
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)          # ì€ë‹‰ h â†’ í‰ê·  Î¼
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)      # ì€ë‹‰ h â†’ ë¡œê·¸ ë¶„ì‚° log(ÏƒÂ²)

        # ğŸ”¶ Decoder
        self.fc2 = nn.Linear(latent_dim, hidden_dim)            # z â†’ ì€ë‹‰ h
        self.fc3 = nn.Linear(hidden_dim, input_dim)             # ì€ë‹‰ h â†’ ë³µì›ëœ x

```

ë‹¤ìŒì€ Variational Autoencoderì˜ í•™ìŠµ ë£¨í”„ ì¼ë¶€ì´ë‹¤.

```python

# Reconstruction Loss

recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')

# KL Divergence

kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

# ELBO (Negative)

loss = recon_loss + kl_loss
```

ì´ ì½”ë“œì—ì„œ ê°ê°ì˜ ì†ì‹¤ í•­ì´ ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ ì„¤ëª…í•˜ì‹œì˜¤.
ë˜í•œ ì´ ì†ì‹¤ì˜ ìˆ˜í•™ì  ìµœì¢… í˜•íƒœë¥¼ ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì“°ì‹œì˜¤.
